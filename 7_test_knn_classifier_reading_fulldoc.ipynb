{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test KNN classifier on classifying every word in all 100 ground-truth documents (documents from which experts manually extracted polymer names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import spacy\n",
    "import sklearn\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import scipy as sp\n",
    "import cPickle as pkl\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import gensim, logging\n",
    "# This log shows progress and is very useful\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#reload(sys)\n",
    "#sys.setdefaultencoding(\"utf-8\")\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load spacy for candidate processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Adding these into the vocabulary\n",
    "nlp.vocab[u\"diblock\"].is_stop = False\n",
    "nlp.vocab[u\"g/mol\"].is_stop = False\n",
    "nlp.vocab[u\"kg/mol\"].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "def connect_to_db():\n",
    "    database = \"../../db/sentences.db\"\n",
    "    conn = create_connection(database)\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DB\n",
    "def create_connection(db_file):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    " \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a string is a number\n",
    "def is_number(n):\n",
    "    n=n.strip().rstrip(\" g\").rstrip(\" K\")\n",
    "    # Strip units\n",
    "    units = [u'S/cm',u'mC/cm2',u'cm2/C',u'°C',u'°C/min',u'mL',u'mol',u'mmol',u'mg/mL',u\"mL\",\n",
    "             u'eV',u'°',u'g/mol',u'mL/min',u'μL',u'μm',u'cm–1',u'cm–2',u'mol−1',u\"MPa1/2\",u\"cm2\", u\"V−1\", u\"s−1\",u\"MHz\"]\n",
    "\n",
    "    #for unit in units:\n",
    "    #    n = n.rstrip(unit)\n",
    "    #if n == \"\":\n",
    "    #    return False\n",
    "    un = u'%s' % (n)\n",
    "    if any(unit in un for unit in units):\n",
    "        return True\n",
    "\n",
    "    # Signs\n",
    "    signs = [u\"+\",u\"<\",u\">\",u\"∼\",u\"~\",u\"%\",u\"±\",u\"≥\",u\"≤\",u\"≈\",u\"−\",u\"-\"]\n",
    "    try:\n",
    "        float(n)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        if len(n)>1 and n[0] in signs:\n",
    "            return True\n",
    "        if len(n)>0 and n[len(n)-1]==\"%\":\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "# Check if a string is a range   \n",
    "def is_range(n):\n",
    "    n= n.strip().rstrip(\" g\").rstrip(\" K\")\n",
    "    if n.find(u\"–\") != -1:\n",
    "        bits = n.split(u\"–\")\n",
    "    elif n.find(u\"−\")!=-1:\n",
    "        bits = n.split(u\"−\")\n",
    "    elif n.find(u\"-\")!=-1:\n",
    "        bits = n.split(u\"-\")\n",
    "    else:\n",
    "        return False\n",
    "    for bit in bits:\n",
    "        if is_number(bit) == False:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Check if the token is made up of numbers separated by punctuation\n",
    "def is_all_numbers(n):\n",
    "    bits = re.split(' |:|;|-|,|;|\\.',n)\n",
    "    #bits = re.split(\",.:\")\n",
    "    for bit in bits:\n",
    "        if is_number(bit) == False:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# First use vectors as features\n",
    "def refine_candidate(candidate):\n",
    "    # Top context words in DB\n",
    "    frequent_context_words = [\"weight\",\"material\",\"system\",\"chains\",\"samples\", \"systems\",\"Tg\",\"weights\",\"comparison\",\n",
    "                              \"chromatography\",\"Mn\",\"THF\",\"toluene\",\"GPC\",\"chloroform\",\"index\",\"Column\",\"columns\",\n",
    "                              \"standards\",\"reference\",\"segments\",\"polydispersity\",\"substrate\",\"block\",\"components\",\n",
    "                              \"permeation\",\"component\",\"Mw\",\"bulk\",\"standard\",\"calibration\",\"dynamics\",\"cross-linked\",\n",
    "                              \"cells\",\"domains\",\"segment\",\"mixtures\",\"densities\",\"substrates\",\"well-defined\",\"silica\",\n",
    "                              \"SEC\",\"particles\",\"compositions\",\"surfaces\",\"linear\",\"blend\",\"blends\"]\n",
    "    \n",
    "    # Polymers that exist in the English dictionary and should not be removed as unrelated English words\n",
    "    common_polys = ['polyethylene', 'polyurethane', 'polypropylene', 'polyester', 'PS', 'polystyrene', 'PLA', 'PI', \n",
    "                    'PET', 'PVP', 'PEG', 'cellulose', 'PAN', 'methyl'] #These are polymers that could appear within spacy vocab\n",
    "    common_polys = [polymer.lower() for polymer in common_polys] \n",
    "    \n",
    "    # Filter out junk values; junk values are\n",
    "    # a. Numbers or range of numers\n",
    "    # b. Long sentences withing parenthesis which weren't broken by our special tokenizer, this is done by recognizing\n",
    "    #    words inside such sentence as an English vocabulary \n",
    "    # c. Remove top context words that are related to polymers in the database but have been marked as \"not polymers\" (count>20)\n",
    "     \n",
    "    if len(candidate)==1:\n",
    "        return \"ignore\"\n",
    "        \n",
    "    # Remove words that are in the English vocabulary\n",
    "    if (candidate in nlp.vocab) and candidate.lower() not in common_polys:\n",
    "        return \"ignore\"\n",
    "    \n",
    "    # We can get the count on the fly but didn't use it here, we used the count in our database instead\n",
    "    # We remove the most frequent context words found in the annotated data\n",
    "    # vocab_obj = model.wv.vocab[candidate]\n",
    "    # freq= vocab_obj.count\n",
    "    if candidate.lower() in frequent_context_words:\n",
    "        return \"ignore\"\n",
    "            \n",
    "    # Remove numbers\n",
    "    if is_number(candidate) == True or is_range(candidate) == True or is_all_numbers(candidate) == True:\n",
    "        return \"ignore\"\n",
    "\n",
    "    # Remove long sentences within parenthesis\n",
    "    junk = False\n",
    "\n",
    "    items = re.split(' |:|;|-',candidate)\n",
    "    for item in items:\n",
    "        #Removing items that are sentences within  parenthesis\n",
    "        if item != \"poly\" and is_number(item)==False and (\"standard\" in item or (item in nlp.vocab and item not in common_polys)):\n",
    "            junk = True\n",
    "            break\n",
    "    if junk is True:\n",
    "        return \"ignore\"\n",
    "    \n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up token\n",
    "def cleanup_token(tkn):\n",
    "    new_tkn = tkn.strip(' .,.;')\n",
    "    if len(new_tkn)>=2 and new_tkn[0] == '(' and new_tkn[len(new_tkn)-1]==')':\n",
    "        new_tkn=new_tkn.rstrip(')').lstrip('(')\n",
    "    return new_tkn\n",
    "\n",
    " # Filter using annotated data from pipeline or full documents\n",
    "def get_sentences_from_db(conn, doi):\n",
    "    sentences = []\n",
    "    cur = conn.cursor()\n",
    "    cur.execute('select sentence from sentences where docid=\"%s\"' % doi) \n",
    "    rows = cur.fetchall()\n",
    "    for row in rows:\n",
    "        sentences.append(row[0]) \n",
    "    return sentences  \n",
    "\n",
    "\n",
    "def read_documents(conn,ground_truth_dois):\n",
    "    global full_list_of_candidates\n",
    "    global seen_words\n",
    "    global full_list_of_probabilities\n",
    "    f = open(ground_truth_dois)\n",
    "    lines = f.readlines()\n",
    "    polymers = {}\n",
    "    probabilities = {}\n",
    "    count = 0\n",
    "    #seen_words = []\n",
    "    for line in lines:\n",
    "        doi = line.strip(\"\\n\")\n",
    "        polymers[doi]=[]\n",
    "        probabilities[doi]=[]\n",
    "        sentences = get_sentences_from_db(conn,doi)\n",
    "        words = []\n",
    "        \n",
    "        word_vectors = []\n",
    "        for sentence in sentences:\n",
    "            # Get spacy tokens and where tokens match - check POS to keep only nouns\n",
    "            # Create dictionary to check in\n",
    "            check_pos = {}\n",
    "            doc = nlp(sentence.rstrip(\".\"))\n",
    "            for token in doc:\n",
    "                check_pos[token.text] = token.pos_\n",
    "            \n",
    "            \n",
    "            tokens = re.split(r\"\\s+(?=[^()]*(?:\\(|$))\", sentence.rstrip(\".\"))\n",
    "            new_tokens = list(map(lambda token:cleanup_token(token),tokens))\n",
    "            for new_token in new_tokens:\n",
    "                if new_token in seen_words or new_token in full_list_of_candidates:\n",
    "                    continue\n",
    "                    \n",
    "                if new_token in check_pos.keys():\n",
    "                    if not(check_pos[new_token] == \"NOUN\" or check_pos[new_token] == \"PROPN\"):\n",
    "                        continue   \n",
    "                        \n",
    "                if new_token in vocabulary:\n",
    "                    if new_token == \"\":\n",
    "                        continue\n",
    "                    refined_token = refine_candidate(new_token)\n",
    "                    if refined_token == \"ignore\":\n",
    "                        continue\n",
    "                    if is_number(new_token)==True:\n",
    "                        continue\n",
    "                    if is_number(new_token[:-1].strip())==True:\n",
    "                        continue\n",
    "                    if is_number(new_token[1:].strip())==True:\n",
    "                        continue\n",
    "                    if len(new_token)>2 and is_number(new_token[1:-1].strip())==True:\n",
    "                        continue\n",
    "                    if is_range(new_token)==True:\n",
    "                        continue\n",
    "                    if is_all_numbers(new_token)==True:\n",
    "                        continue\n",
    "                    # Keep track of seen vectors sp we don't have to read them again.\n",
    "                    # Alternatively, I could get a list of unique tokens and generate the vecors\n",
    "                    words.append(new_token)\n",
    "                    seen_words.append(new_token)\n",
    "                    word_vectors.append(model.wv[new_token])\n",
    "                \n",
    "        #predicted_labels = best_knn_clf.predict_proba(word_vectors)\n",
    "        predicted_labels = best_knn_clf.predict(word_vectors)\n",
    "        #print predicted_labels\n",
    "        count_words = 0\n",
    "        for label in predicted_labels:\n",
    "            #if label == 1:\n",
    "            polymers[doi].append(words[count_words])\n",
    "            probabilities[doi].append(label)\n",
    "            #probabilities[doi].append([label[0],label[1]])\n",
    "            #print words[count_words]\n",
    "            count_words = count_words + 1\n",
    "        print \"Doi #%s: %s and number of candidates: %s\" % (count, doi, len(polymers[doi]))\n",
    "        count = count + 1\n",
    "\n",
    "        # Extend full list of candidates by polymers\n",
    "        full_list_of_candidates.extend(polymers[doi])\n",
    "        full_list_of_probabilities.extend(probabilities[doi])\n",
    "        \n",
    "        #full_list_of_candidates = list(set(full_list_of_candidates))\n",
    "    return polymers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a list of unique candidates and a list of seen words because words have the same vectors\n",
    "full_list_of_candidates = []\n",
    "full_list_of_probabilities = []\n",
    "seen_words = []\n",
    "# Load model\n",
    "best_knn_clf = pickle.load(open('pipeline_refined_model.clf', 'rb')) # used for paper\n",
    "#best_knn_clf = pickle.load(open('best_knn_model.clf')) # Trying this classifier instead (pipeline unrefined =  best fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre trained gensim model\n",
    "model = gensim.models.Word2Vec.load('../../models/gensim_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "word_vectors = model.wv\n",
    "# Get vocab\n",
    "vocabulary = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects to db\n",
    "connection = connect_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Time execution\n",
    "start_time = time.time()\n",
    "\n",
    "# Get candidates from the ground truth documents\n",
    "candidates = read_documents(connection, '../../data/ground_truth_dois.txt')\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print \"Read all documents in %.2f\" % (end_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all documents in 5761.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reload(sys)\n",
    "#sys.setdefaultencoding('utf8')\n",
    "#f = open('results.txt','w+')\n",
    "#for candidate in full_list_of_candidates:\n",
    "#    line = u'%s\\n' % candidate\n",
    "#    f.write(line)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open('prediction_probabilities.txt','w+')\n",
    "f = open('noun_prediction.txt','w+')\n",
    "for count in range(len(full_list_of_candidates)):\n",
    "    polymer = full_list_of_candidates[count].encode('utf-8')\n",
    "    label = full_list_of_probabilities[count]\n",
    "    line = '%s\\t%s\\n' % (label, polymer)\n",
    "    #prob_no = full_list_of_probabilities[count][0]\n",
    "    #prob_yes = full_list_of_probabilities[count][1]\n",
    "    #line = '%s\\t%s\\t%s\\n' % (prob_no, prob_yes,polymer)\n",
    "    f.write(line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_to_csv(dictionary, csv_basename):\n",
    "    csv_filename = csv_basename \n",
    "    with open(csv_filename, 'w+') as resultFile:\n",
    "    \n",
    "        wr = csv.writer(resultFile, dialect='excel')\n",
    "        for key, values in dictionary.iteritems():\n",
    "            #if len(values) == 0:\n",
    "            #    continue\n",
    "            polymers = []\n",
    "            polymers.append(key)\n",
    "            for value in values:\n",
    "                polymers.append(value.encode('utf-8'))\n",
    "            wr.writerow(polymers)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_results_to_csv(candidates,'../candidates/perdocformat/classifier_candidates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
