{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification  - Testing K Nearest Neighbors\n",
    "## Try this model with word vectors of candidates generated via word2vec\n",
    "## First test on candidates from the 50-document test set\n",
    "## Then test the KNN classifier on polyNER's candidates from the 100 ground truth documents \n",
    "## Finally test on classifying a test set of manually extracted polymers from the ground truth dcuments (only true positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from   __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "from   matplotlib.colors import ListedColormap\n",
    "from   sklearn import neighbors, datasets\n",
    "import gensim, logging\n",
    "\n",
    "from   sklearn import svm\n",
    "from   sklearn.svm import SVR\n",
    "from   sklearn.svm import SVC\n",
    "from   sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import sklearn\n",
    "import spacy\n",
    "\n",
    "from   sklearn.model_selection import train_test_split\n",
    "from   sklearn.model_selection import GridSearchCV\n",
    "from   sklearn.metrics import classification_report\n",
    "from   sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from   sklearn.manifold import TSNE\n",
    "\n",
    "from   sklearn.ensemble import RandomForestRegressor\n",
    "from   sklearn.model_selection import train_test_split, GridSearchCV, ShuffleSplit\n",
    "import scipy as sp\n",
    "import cPickle as pkl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from   sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# This log shows progress and is very useful\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Load spacy for candidate processing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Adding these into the vocabulary\n",
    "nlp.vocab[u\"diblock\"].is_stop = False\n",
    "nlp.vocab[u\"g/mol\"].is_stop   = False\n",
    "nlp.vocab[u\"kg/mol\"].is_stop  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to db\n",
    "def connect_to_db():\n",
    "    database = \"../../db/sentences.db\"\n",
    "    conn = create_connection(database)\n",
    "    return conn\n",
    "\n",
    "def create_connection(db_file):\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        return conn\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get candidate labels\n",
    "def get_candidate_labels_from_db(conn, process_full_document_flag):\n",
    "    # read candidates from just fasttext-classified sentences\n",
    "    pipeline_polymers = readcsv('../candidates/perdocformat/classifier_pipeline_candidates.csv')\n",
    "    \n",
    "    # Store candidates in dict\n",
    "    candidates = {}\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"select distinct polymer, is_poly from polymer_ner_evaluation\") \n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    for row in rows:\n",
    "        if process_full_document_flag:\n",
    "            candidates[row[0]] = row[1]\n",
    "        else:\n",
    "            if row[0] in pipeline_polymers:\n",
    "                candidates[row[0]] = row[1]\n",
    "    return candidates\n",
    "\n",
    "# Read candidate input files\n",
    "def readcsv(ifile):\n",
    "    polymer_candidates = []\n",
    "    with open(ifile, 'rb') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        for row in csvreader:\n",
    "            new_row = [unicode(cell, 'utf-8') for cell in row]\n",
    "            #new_ro = [cleanup_poly(x) for x in new_row]\n",
    "            polymer_candidates.extend(new_row[1:])\n",
    "    return list(set(polymer_candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_testing_data(features, target):\n",
    "    # Dataset, training and testing datasets\n",
    "    X = np.asarray(features)\n",
    "    y = np.asarray(target)\n",
    "    Xtrain, Xtest, ytrain, ytest = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=0)\n",
    "    return Xtrain, Xtest, ytrain, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics - I think I can do that with scikit learn\n",
    "def metrics(predicted, actual):  \n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    true_neg = 0\n",
    "    num_pos = 0\n",
    "    num_polys = 0\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == 1 and actual[i] == 1:\n",
    "            true_pos = true_pos + 1\n",
    "        elif predicted[i] == 1 and actual[i] == 0:\n",
    "            false_pos = false_pos + 1\n",
    "        elif predicted[i] == 0 and actual[i] == 0:\n",
    "            true_neg = true_neg + 1\n",
    "        elif predicted[i] == 0 and actual[i] == 1:\n",
    "            false_neg = false_neg + 1\n",
    "\n",
    "    print('    Test points:     %d' % len(predicted))\n",
    "    print('    True positives:  %d' % true_pos)\n",
    "    print('    False positive:  %d' % false_pos)\n",
    "    print('    True negatives:  %d' % true_neg)\n",
    "    print('    False negatives: %d' % false_neg)\n",
    "    if false_pos+true_pos > 0:\n",
    "        precision = true_pos/(true_pos+false_pos)\n",
    "\n",
    "        recall = true_pos/(true_pos+false_neg)\n",
    "        accuracy = (true_pos + true_neg)/(true_pos+true_neg+false_pos+false_neg)\n",
    "        f1score = 2/((1/recall)+(1/precision))\n",
    "    else: #FIXME\n",
    "        precision = 0\n",
    "        recall = 1\n",
    "        f1score = -1 #FIXME: check \n",
    "    print('    Precision:       %.3f' % precision)\n",
    "    print('    Recall:          %.3f' % recall)\n",
    "    #print \"Accuracy: \", accuracy / just to check my metrics function was correct\n",
    "    print('    F-1 score:       %.3f' %f1score)\n",
    "    #print clf.score(predicted,actual)\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if a string is a number\n",
    "def is_number(n):\n",
    "    try:\n",
    "        float(n)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "# First use vectors as features\n",
    "def refine_candidate(candidate):\n",
    "    # Top context words in DB\n",
    "    frequent_context_words = [\"weight\",\"material\",\"system\",\"chains\",\"samples\", \"systems\",\"Tg\",\"weights\",\"comparison\",\"chromatography\",\"Mn\",\"THF\",\"toluene\",\"GPC\",\"chloroform\",\"index\",\"Column\",\"columns\",\"standards\",\"reference\",\"segments\",\"polydispersity\",\"substrate\",\"block\",\"components\",\"permeation\",\"component\",\"Mw\",\"bulk\",\"standard\",\"calibration\",\"dynamics\",\"cross-linked\",\"cells\",\"domains\",\"segment\",\"mixtures\",\"densities\",\"substrates\",\"well-defined\",\"silica\",\"SEC\",\"particles\",\"compositions\",\"surfaces\",\"linear\"]\n",
    "    \n",
    "    common_polys = ['polyethylene', 'polyurethane', 'polypropylene', 'polyester', 'PS', 'polystyrene', 'PLA', 'PI', 'PET', 'PVP', 'PEG', 'cellulose', 'PAN', 'methyl'] #These are polymers that could appear within spacy vocab\n",
    "    common_polys = [polymer.lower() for polymer in common_polys] \n",
    "\n",
    "    # Filter out junk values\n",
    "    junk_vals = []\n",
    "        \n",
    "    if (candidate in nlp.vocab) and candidate.lower() not in common_polys:\n",
    "        return \"ignore\"\n",
    "    vocab_obj = model.wv.vocab[candidate]\n",
    "    freq= vocab_obj.count\n",
    " \n",
    "    if candidate in frequent_context_words:\n",
    "        return \"ignore\"\n",
    "            \n",
    "    junk = False\n",
    "    items = re.split(' |:|;|-',candidate)\n",
    "    for item in items:\n",
    "        #Removing items that are sentences within  parenthesis\n",
    "        if item != \"poly\" and is_number(item)==False and (\"standard\" in item or (item in nlp.vocab and item not in common_polys)):\n",
    "            junk = True\n",
    "            break\n",
    "\n",
    "    if junk is True:\n",
    "        return \"ignore\"\n",
    "    \n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First use vectors as features\n",
    "def get_word_vectors_as_features(polymer_candidates, refined_candidates_only_flag):\n",
    "    features = {}\n",
    "    Xl = [] # vectors\n",
    "    yl = [] # target (is_poly=0:1?)\n",
    "    ll = [] # labels\n",
    "    for k, v in polymer_candidates.iteritems():\n",
    "        if not refined_candidates_only_flag:\n",
    "            ll.append(k)\n",
    "            features[k] = word_vectors[k]\n",
    "            Xl.append(features[k])\n",
    "            yl.append(polymer_candidates[k])\n",
    "        else:\n",
    "            new_k = refine_candidate(k)\n",
    "            if new_k != \"ignore\":\n",
    "                ll.append(k)\n",
    "                features[k] = word_vectors[k]\n",
    "                Xl.append(features[k])\n",
    "                yl.append(polymer_candidates[k])\n",
    "                \n",
    "    return Xl, yl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select data to work with, subject to these flags:\n",
    "#   process_full_document_flag:   Work with all sentences from a document (True) or only fasttext selected sentences (False)\n",
    "#   refined_candidates_only_flag: Refine the candidates (remove english words, remove frequent context words etc): True or False\n",
    "#   use_word_vector_flag:         Use word vectors (True) or similarity to PS scores (False)\n",
    "def select_fulldoc_refined_featuretype(conn, process_full_document_flag, refined_candidates_only_flag, use_word_vector_flag):\n",
    "    poly_candidates = get_candidate_labels_from_db(conn, process_full_document_flag)\n",
    "\n",
    "    if use_word_vector_flag:\n",
    "        X, y = get_word_vectors_as_features(poly_candidates, refined_candidates_only_flag)\n",
    "    else:\n",
    "        X, y = get_similarity_scores_as_features(poly_candidates, refined_candidates_only_flag)\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all classifiers on a particular type of input\n",
    "def run_knn_classifiers(X,y):\n",
    "    selected_model = 'K Nearest Neighbor'\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = get_training_testing_data(X,y)\n",
    "    \n",
    "    print('K Nearest Neighbor:')\n",
    "    f1_score, clf = knn(X_train, X_test, y_train, y_test)\n",
    "    \n",
    "\n",
    "    return(selected_model, f1_score, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre trained gensim model\n",
    "model = gensim.models.Word2Vec.load('../../models/gensim_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word vectors\n",
    "word_vectors = model.wv\n",
    "# Get vocab\n",
    "vocabulary = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define KNN, SVC, and RF models\n",
    "\n",
    "def knn(Xtrain,Xtest, ytrain, ytest):\n",
    "    # Number of neighbors 5 seems to work best\n",
    "    n_neighbors = 5\n",
    "\n",
    "    #for weights in ['uniform', 'distance']:\n",
    "    weights = 'uniform'\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "\n",
    "    clf.fit(Xtrain,ytrain)\n",
    "\n",
    "    y_predicted = clf.predict(Xtest)\n",
    "    f1s = metrics(y_predicted,ytest)\n",
    "    \n",
    "    return f1s, clf\n",
    "\n",
    "\n",
    "def run_all_knn_models(connection, process_full_document_flag, refined_candidates_only_flag, use_word_vector_flag):\n",
    "    print('Running all classifiers, with %s candidates; %s; %s' % \n",
    "      ('refined' if refined_candidates_only_flag else 'unrefined',\n",
    "       'word vectors' if use_word_vector_flag else 'score vectors',\n",
    "       'full documents' if process_full_document_flag else 'classified sentences'))\n",
    "\n",
    "    X, y = select_fulldoc_refined_featuretype(connection, process_full_document_flag, refined_candidates_only_flag, use_word_vector_flag)\n",
    "    \n",
    "    best_model_name, max_f1_score, best_model_bin = run_knn_classifiers(X, y)\n",
    "    key = ('fulldoc' if process_full_document_flag else 'classified_sentences') + '_' + ('refined' if refined_candidates_only_flag else 'unrefined') + '_' + ('words' if use_word_vector_flag else 'scores')\n",
    "    results[key] = [best_model_name, max_f1_score, best_model_bin]\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and run knn models four times\n",
    "\n",
    "Run each of the classifiers for the following 2 x 2 x 2 = 8 configurations:\n",
    "1. (all docs vs. fasttext-selected)\n",
    "1. (refined vs. unrefined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a list of best models and best F scores\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connects to db\n",
    "connection = connect_to_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_full_document_flag   = True\n",
    "refined_candidates_only_flag = False\n",
    "use_word_vector_flag         = True\n",
    "\n",
    "run_all_knn_models(connection, process_full_document_flag, refined_candidates_only_flag, use_word_vector_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_full_document_flag   = False\n",
    "refined_candidates_only_flag = False\n",
    "use_word_vector_flag         = True\n",
    "\n",
    "run_all_knn_models(connection, process_full_document_flag, refined_candidates_only_flag, use_word_vector_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_full_document_flag   = True\n",
    "refined_candidates_only_flag = True\n",
    "use_word_vector_flag         = True\n",
    "\n",
    "run_all_knn_models(connection, process_full_document_flag, refined_candidates_only_flag, use_word_vector_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_full_document_flag   = False\n",
    "refined_candidates_only_flag = True\n",
    "use_word_vector_flag         = True\n",
    "\n",
    "run_all_knn_models(connection, process_full_document_flag, refined_candidates_only_flag, use_word_vector_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_score = 0.0\n",
    "\n",
    "for r in results:\n",
    "    (method, f_score, clf) = results[r]\n",
    "    if f_score > max_score:\n",
    "        max_method = method\n",
    "        max_config = r\n",
    "        max_score = f_score\n",
    "        max_clf = clf\n",
    "    print('%s: KNN classifier achieves the best f-score of %.3f' % (r, f_score))\n",
    "print('\\nBest overall KNN score was %.3f for %s ' %(max_score, max_config))\n",
    "filename = 'best_knn_model.clf'\n",
    "pickle.dump(max_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that the classifier has been tested on labeled candidates, \n",
    "# it needs to be tested using candidates from the ground truth\n",
    "# I had the option to get candidates from full doc, but polyNER\n",
    "# downsamples the sentences, to evaluate polyNER, we should really\n",
    "# only consider pipeline candidates\n",
    "def get_ground_truth_candidates(full_doc):\n",
    "    # read candidates from just fasttext sentences\n",
    "    ground_truth_fulldoc_polymers = readcsv('../candidates/perdocformat/groundtruth_fulldocument_candidates.csv')\n",
    "    \n",
    "    # read candidates from just fasttext sentences\n",
    "    ground_truth_pipeline_polymers = readcsv('../candidates/perdocformat/groundtruth_pipeline_candidates.csv')\n",
    "    \n",
    "    # Store candidates in dict\n",
    "    candidates = {}\n",
    "    \n",
    "    if full_doc == 1:\n",
    "        for poly in ground_truth_fulldoc_polymers:\n",
    "            candidates[poly] = -1\n",
    "    else:\n",
    "        for poly in ground_truth_pipeline_polymers:\n",
    "            candidates[poly] = -1\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vectors for polymer candidates (configurations are fulldoc vs pipeline, refined vs. unrefined)\n",
    "def get_ground_truth_candidates_word_vectors_as_features(polymer_candidates,refined):\n",
    "    features = {}\n",
    "    Xl = [] # vectors\n",
    "    ll = []\n",
    "    for k, v in polymer_candidates.iteritems():\n",
    "        if refined == 0:\n",
    "            ll.append(k)\n",
    "            features[k] = word_vectors[k]\n",
    "            Xl.append(features[k])\n",
    "        else:\n",
    "            new_k = refine_candidate(k)\n",
    "            if new_k != \"ignore\":\n",
    "                ll.append(k)\n",
    "                features[k] = word_vectors[k]\n",
    "                Xl.append(features[k])\n",
    "    return Xl, ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_ground_truth_candidates_fulldocument_refined(full_document,refined_candidates):\n",
    "    # Use entire document as opposed to fasttext selected sentences\n",
    "    if full_document == 1:\n",
    "        poly_candidates = get_ground_truth_candidates(1)\n",
    "    else:\n",
    "        poly_candidates = get_ground_truth_candidates(0)\n",
    "        \n",
    "    if refined_candidates == 0:\n",
    "        X, l = get_ground_truth_candidates_word_vectors_as_features(poly_candidates,0)\n",
    "    elif refined_candidates == 1:\n",
    "        X, l = get_ground_truth_candidates_word_vectors_as_features(poly_candidates,1)\n",
    "    return X,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get external candidates = Get vectors for strings provided in a list that may or may not have a vector in the word2vec model\n",
    "# Save a file with words that are out of vocabulary \n",
    "def get_external_candidates_vectors(input_file):\n",
    "    \n",
    "    ifl = open(input_file,'rb')\n",
    "    output_file = input_file.split(\".txt\")[0]+\"_oovocab.txt\"\n",
    "    \n",
    "    ofl = open(output_file,'w+')\n",
    "    \n",
    "    candidates = []\n",
    "    candidate_vectors = []\n",
    "    oovocab = []\n",
    "    lines = ifl.readlines()\n",
    "    for line in lines:\n",
    "        #print line\n",
    "        candidate = u'%s' % cleanup_token(line.strip().strip(\"\\n\").decode('utf-8'))\n",
    "        #print candidate\n",
    "        if candidate not in vocabulary:\n",
    "            if candidate not in oovocab:\n",
    "                oovocab.append(candidate)\n",
    "                ofl.write(candidate.encode('utf-8')+\"\\n\")\n",
    "        else:\n",
    "            candidates.append(candidate)\n",
    "            candidate_vectors.append(model.wv[candidate])\n",
    "    ifl.close()\n",
    "    ofl.close()\n",
    "    print \"Number of words out of vocab: \", len(oovocab)   # 1   in FastText 500 candidates\n",
    "    print \"Number of word in vocabulary:\", len(candidates) # 499 in FastText 500 candidates\n",
    "    return candidate_vectors, candidates\n",
    "\n",
    "# Extend candidate word vectors with a second list of vectors and strings\n",
    "# This function extend the current list of candidates with vectors from FastText CG2 word embedding model candidates\n",
    "def extend_candidates(vectors1, names1, vectors2, names2):\n",
    "    vectors1.extend(vectors2)\n",
    "    names1.extend(names2)\n",
    "\n",
    "# Strip tokens\n",
    "def cleanup_token(tkn):\n",
    "    new_tkn = tkn.strip().strip(',').strip('.')\n",
    "    if len(new_tkn)>=2 and new_tkn[0] == '(' and new_tkn[len(new_tkn)-1]==')':\n",
    "        new_tkn=new_tkn.rstrip(')').lstrip('(')\n",
    "    return new_tkn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted polymers from classifying ground truth polymers\n",
    "def get_ground_truth_predicted_polymers(predicted_labels,names,filename):\n",
    "    f = open(filename,'w+')\n",
    "    for l in range(len(predicted_labels)):\n",
    "        if predicted_labels[l]==1:\n",
    "            polyname = names[l].encode('utf-8')\n",
    "            f.write(polyname+\"\\n\")\n",
    "            #f.write(u\"%s\\n\" % names[l].decode('utf-8'))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select unrefined candidates from the pipeline (generated the best classifier according to knn performance )\n",
    "# fulldoc = 0, refined = 0\n",
    "X2, labels2 = select_ground_truth_candidates_fulldocument_refined(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vectors for FastText (string) candidates\n",
    "# It turns out this doesn't help results (likely because vectors aren't computed in the same manner,\n",
    "# otherwise, gensim would have also found more of these candidates)\n",
    "# FTX, FTlabels = get_external_candidates_vectors('../candidates/listformat/FT_candidates_500.txt')\n",
    "#extend_candidates(X2, labels2, FTX, FTlabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_polys2 = max_clf.predict(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used below when we extended candidates with FastText candidates\n",
    "# get_GT_predicted_polymers(predicted_polys2,labels2,\"classifierFT_pipeline_unrefined.txt\") \n",
    "get_ground_truth_predicted_polymers(predicted_polys2,labels2,\"knn_pipeline_unrefined.txt\")\n",
    "# FIXME: Import evaluation script here so that it can be done in this notebook as well\n",
    "# instead of one notebook that evaluates several candidate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that in the paper we used classifier 4 (pipeline refined) because in general the last step\n",
    "# yielded the best precison and recall (see draft knn classifier notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ground truth polymer names and try to classify them with the best classifier\n",
    "GTX, GTlabels = get_external_candidates_vectors('../ground_truth/ground_truth_list_format.txt')\n",
    "predicted_ground_truth_polys = max_clf.predict(GTX)\n",
    "actual = [1 for count in range(len(predicted_ground_truth_polys))]\n",
    "metrics(predicted_ground_truth_polys,actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of words out of vocab:  104\n",
    "Number of word in vocabulary: 501\n",
    "    Test points:     501\n",
    "    True positives:  473\n",
    "    False positive:  0\n",
    "    True negatives:  0\n",
    "    False negatives: 28\n",
    "    Precision:       1.000\n",
    "    Recall:          0.944\n",
    "    F-1 score:       0.971"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
